{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BPE on a toy text example\n",
    "\n",
    "code adapted from: https://leimao.github.io/blog/Byte-Pair-Encoding/\n",
    "bpe algorithm: https://web.stanford.edu/~jurafsky/slp3/2.pdf (2.4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab = defaultdict(<class 'int'>, {'T h e </w>': 2, 'a i m s </w>': 1, 'f o r </w>': 4, 't h i s </w>': 1, 's u b j e c t </w>': 1, 'i s </w>': 2, 's t u d e n t s </w>': 1, 't o </w>': 2, 'd e v e l o p </w>': 1, 'a n </w>': 1, 'u n d e r s t a n d i n g </w>': 1, 'o f </w>': 2, 't h e </w>': 2, 'm a i n </w>': 1, 'a l g o r i t h m s </w>': 1, 'u s e d </w>': 2, 'i n </w>': 3, 'n a t u r a l </w>': 1, 'l a n g u a g e </w>': 3, 'p r o c e s s i n g , </w>': 1, 'u s e </w>': 2, 'a </w>': 1, 'd i v e r s e </w>': 1, 'r a n g e </w>': 1, 'a p p l i c a t i o n s </w>': 1, 'i n c l u d i n g </w>': 1, 't e x t </w>': 1, 'c l a s s i f i c a t i o n , </w>': 1, 'm a c h i n e </w>': 1, 't r a n s l a t i o n , </w>': 1, 'a n d </w>': 3, 'q u e s t i o n </w>': 1, 'a n s w e r i n g . </w>': 1, 'T o p i c s </w>': 1, 'b e </w>': 1, 'c o v e r e d </w>': 1, 'i n c l u d e </w>': 1, 'p a r t - o f - s p e e c h </w>': 1, 't a g g i n g , </w>': 1, 'n - g r a m </w>': 1, 'm o d e l l i n g , </w>': 1, 's y n t a c t i c </w>': 1, 'p a r s i n g </w>': 1, 'd e e p </w>': 1, 'l e a r n i n g . </w>': 1, 'p r o g r a m m i n g </w>': 1, 'P y t h o n , </w>': 1, 's e e </w>': 1, 'm o r e </w>': 1, 'i n f o r m a t i o n </w>': 1, 'o n </w>': 1, 'i t s </w>': 1, 'w o r k s h o p s , </w>': 1, 'a s s i g n m e n t s </w>': 1, 'i n s t a l l a t i o n </w>': 1, 'a t </w>': 1, 'h o m e . </w>': 1})\n",
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'T': 3, 'h': 11, 'e': 39, '</w>': 73, 'a': 38, 'i': 37, 'm': 12, 's': 34, 'f': 9, 'o': 29, 'r': 22, 't': 29, 'u': 14, 'b': 2, 'j': 1, 'c': 13, 'd': 15, 'n': 45, 'v': 3, 'l': 16, 'p': 11, 'g': 22, ',': 7, 'x': 1, 'q': 1, 'w': 2, '.': 3, '-': 3, 'y': 2, 'P': 1, 'k': 1})\n",
      "Number of tokens: 31\n",
      "==========\n",
      "defaultdict(<class 'int'>, {('T', ' '): 3, (' ', 'h'): 8, ('h', ' '): 8, ('a', ' '): 27, (' ', 'i'): 11, ('i', ' '): 16, (' ', 'm'): 3, ('f', ' '): 7, (' ', 'o'): 17, ('o', ' '): 18, ('t', ' '): 16, ('s', ' '): 20, (' ', 'u'): 8, ('u', ' '): 10, (' ', 'b'): 1, ('b', ' '): 2, (' ', 'j'): 1, ('j', ' '): 1, (' ', 's'): 18, (' ', 't'): 10, (' ', 'd'): 3, ('d', ' '): 6, (' ', 'e'): 18, ('e', ' '): 13, (' ', 'v'): 3, ('v', ' '): 3, (' ', 'n'): 22, ('n', ' '): 19, (' ', 'r'): 14, ('r', ' '): 12, (' ', 'f'): 5, ('m', ' '): 6, (' ', 'a'): 18, (' ', 'l'): 10, ('l', ' '): 12, (' ', 'g'): 9, ('g', ' '): 9, ('p', ' '): 7, (' ', 'c'): 5, ('c', ' '): 7, (' ', 'p'): 3, (' ', 'x'): 1, ('q', ' '): 1, (' ', 'w'): 1, ('w', ' '): 2, (' ', '-'): 3, ('-', ' '): 2, (' ', 'y'): 2, ('y', ' '): 2, ('P', ' '): 1, (' ', 'k'): 1, ('k', ' '): 1})\n",
      "Iter: 0\n",
      "Best pair: ('a', ' ')\n",
      "Tokens: defaultdict(<class 'int'>, {})\n",
      "Number of tokens: 0\n",
      "==========\n",
      "defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "text = \"The aims for this subject is for students to develop an understanding of the main algorithms used in natural language processing, for use in a diverse range of applications including text classification, machine translation, and question answering. Topics to be covered include part-of-speech tagging, n-gram language modelling, syntactic parsing and deep learning. The programming language used is Python, see for more information on its use in the workshops, assignments and installation at home.\"\n",
    "\n",
    "def get_vocab(text):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word in text.strip().split():\n",
    "        #note: we use the special token </w> (instead of underscore in the lecture) to denote the end of a word\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This function iterates through all words in the vocabulary and count pair of tokens which are next to each other.\n",
    "\n",
    "EXAMPLE:\n",
    "    word = 'T h e <\\w>'\n",
    "    pairs of tokens in this word [('T', 'h'), ('h', 'e'), ('e', '<\\w>')]\n",
    "    \n",
    "INPUT:\n",
    "    vocab: Dict[str, int]  # The vocabulary, a counter for word frequency\n",
    "    \n",
    "OUTPUT:\n",
    "    pairs: Dict[Tuple[str, str], int] # Word pairs, a counter for pair frequency\n",
    "\n",
    "\"\"\"\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for symbols, freq in vocab.items():\n",
    "        chars = symbols.split()\n",
    "        for i in range(len(chars)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function merges a given pair of tokens in all words in the vocabulary\n",
    "\n",
    "EXAMPLE:\n",
    "    word = 'T h e <\\w>'\n",
    "    pair = ('e', '<\\w>')\n",
    "    word_after_merge = 'T h e<\\w>'\n",
    "    \n",
    "Input:\n",
    "    pair: Tuple[str, str] # the pair of tokens need to be merged\n",
    "    v_in: Dict[str, int]  # vocabulary before merge\n",
    "    \n",
    "Output:\n",
    "    v_out: Dict[str, int] # vocabulary after merge\n",
    "    \n",
    "HINT:\n",
    "    When merging pair ('h', 'e') for word 'Th e<\\w>', the two tokens in this word 'Th' and 'e<\\w>' shouldn't be merged.\n",
    "\n",
    "\"\"\"\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    return v_out\n",
    "\n",
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "print(\"Vocab =\", vocab)\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')\n",
    "\n",
    "#about 100 merges we start to see common words\n",
    "num_merges = 100\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    print (pairs)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After training, used the BPE dictionaries to tokenise sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens = ['understanding</w>', 'algorithms</w>', 'language</w>', 'students</w>', 'subject</w>', 'develop</w>', 'ication', 'lation', 'ing,</w>', 'used</w>', 'inclu', 'ing.</w>', 'aims</w>', 'this</w>', 'main</w>', 'ding</w>', 'ation', 'for</w>', 'and</w>', 'The</w>', 'the</w>', 'use</w>', 'assi', 'gram', 'ing</w>', 'natu', 'nts</w>', 'in</w>', 'is</w>', 'to</w>', 'of</w>', 'pro', 'ver', 'ans', 'par', 'an</w>', 'ang', 'ion', 'ed</w>', 'for', 'e</w>', ',</w>', 's</w>', 'in', 'al', 'st', 'op', 'ic', 'de', 'on', 'or', 'me', 'ss', 't</w>', 'cl', 'ma', 'of', 'ec', 'ag', 'nt', 'ar', 'th', 'at', '.</w>', '</w>', 'e', 's', 't', 'r', 'c', 'p', 'l', 'h', 'm', 'a', 'o', '-', 'n', 'd', 'i', 'w', 'g', 'y', 'x', 'f', 'q', 'u', 'T', 'b', 'P', 'k'] \n",
      "\n",
      "==========\n",
      "Sentence = I like natural language processing!\n",
      "Tokenizing word: I</w>...\n",
      "['</w>']\n",
      "Tokenizing word: like</w>...\n",
      "['l', 'i', 'k', 'e</w>']\n",
      "Tokenizing word: natural</w>...\n",
      "['natu', 'r', 'al', '</w>']\n",
      "Tokenizing word: language</w>...\n",
      "['language</w>']\n",
      "Tokenizing word: processing!</w>...\n",
      "['pro', 'c', 'e', 'ss', 'in', 'g', '</w>']\n",
      "==========\n",
      "Sentence = I like natural languaaage processing!\n",
      "Tokenizing word: I</w>...\n",
      "['</w>']\n",
      "Tokenizing word: like</w>...\n",
      "['l', 'i', 'k', 'e</w>']\n",
      "Tokenizing word: natural</w>...\n",
      "['natu', 'r', 'al', '</w>']\n",
      "Tokenizing word: languaaage</w>...\n",
      "['l', 'ang', 'u', 'a', 'a', 'ag', 'e</w>']\n",
      "Tokenizing word: processing!</w>...\n",
      "['pro', 'c', 'e', 'ss', 'in', 'g', '</w>']\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "    \n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    # iterate over all tokens to find match\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # if no match found in the string, go to next token\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        # collect end position of matches in the string\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # slice for sub-word\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # tokenize this sub-word with tokens remaining\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # tokenize the remaining string\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    return string_tokens\n",
    "\n",
    "\"\"\"\n",
    "This function generates a list of all tokens sorted by their length (1st key) and frequency (2nd key).\n",
    "\n",
    "EXAMPLE:\n",
    "    token frequency dictionary before sorting: {'natural': 3, 'language':2, 'processing': 4, 'lecture': 4}\n",
    "    sorted tokens: ['processing', 'language', 'lecture', 'natural']\n",
    "    \n",
    "INPUT:\n",
    "    token_frequencies: Dict[str, int] # Counter for token frequency\n",
    "    \n",
    "OUTPUT:\n",
    "    sorted_token: List[str] # Tokens sorted by length and frequency\n",
    "\n",
    "\"\"\"\n",
    "def sort_tokens(tokens_frequencies):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return sorted_tokens\n",
    "\n",
    "#display the vocab\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "#sort tokens by length and frequency\n",
    "sorted_tokens = sort_tokens(tokens_frequencies)\n",
    "print(\"Tokens =\", sorted_tokens, \"\\n\")\n",
    "\n",
    "sentence_1 = 'I like natural language processing!'\n",
    "sentence_2 = 'I like natural languaaage processing!'\n",
    "sentence_list = [sentence_1, sentence_2]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    \n",
    "    print('==========')\n",
    "    print(\"Sentence =\", sentence)\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        word = word + \"</w>\"\n",
    "\n",
    "        print('Tokenizing word: {}...'.format(word))\n",
    "        if word in vocab_tokenization:\n",
    "            print(vocab_tokenization[word])\n",
    "        else:\n",
    "            print(tokenize_word(string=word, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
